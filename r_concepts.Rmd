---
title: "R Programming Review"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Frequently missed questions
Using read.csv to find a particular file

~/ for finding a file only needed when you are looking in the home directory.
if you are in the working directory, then yo can just type the file name in quotes

Select the correct ways to  extract a vector from a row in the data frame

df[n, ]
df["rowname", ]

for columns:

df[, n]
df[, "colname"]
df$colname


What input data types can this function take?

jal <- function(x) {
 y = x^2
 return(y)
}

Can take either a numeric or logical value as input without returning error

Conversion between types of data tables in R
 as.dataframe()
 as.data.table()
 as.matrix()
 
 
minimum input for a single layer scatter plot (ie. plot())
  a single numeric vector
  
  this is also the minimum input for hist() and barplot()
  
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Review of statistics

Ho: the sea creature is the monster
Ha: sea creature is not

Null is that there is no difference between our hypothesis and the data

variance: (sigma)^2


Examples:

x = c(5, 6, 4, 6.5)
y = c(2, 10, 7, 3, 5)
x10 <- 10 *c(5, 6, 4, 6.5)

```{r}
x = c(5, 6, 4, 6.5)
y = c(2, 10, 7, 3, 5)
x10 <- 10 *c(5, 6, 4, 6.5)

all <- c(x, y, x10)

sd(x)
sd(y)
sd(x10)

sds
vars


```


What happens to the variance when I scale a measurement from 0→ 100 (mean = 50) to 0→ 1 (mean = 0.5)?

  variance decrases because the range of values is smaller.  Scaling down measurement causes variance to decrease.
  
How does scaling measurements allow for a better comparison of the variances between principal components?

  allows for easier comparison between PCs. let's you account for real variance in   the   data
  
Normal distributions
```{r}
vec <- seq(-1, 1, by = 0.01)
plot(dnorm(vec, mean = 5, sd = 2))

hist(rnorm(5000, mean = 5, sd = 2), breaks = 15)
hist(rnorm(10000, mean = 5, sd = 2), breaks = 30)
abline(hist(rnorm(10000, mean = 5, sd = 2), breaks = 30), v = c(3, 7), col = "red")
```


```{r}
abs(pnorm(2, 5, 2) - pnorm(6, 5, 2)) *100
```

BUild a function to plot the normal distribuition:
Sample size is a big obstacle in creating a normal dist. --> if you don't have enough data points then it is going to be very difficult to conclude that the normal distribution is a good representation of the population

```{r}
x = c(1:100)
mean = 50
stdev = 10
coolnorm <- function(x,mean,stdev){
      y = (1/stdev*sqrt(2*pi))*(exp(1))^(-((x-mean)^2)/(2*stdev)^2)
      return(y)
}
plot(coolnorm(x,mean,stdev))


```


COnfience interval
```{r}
norm <- hist(rnorm(10000, 5, 2))
abline(plot(norm), v = c(3, 7), col = "red")
```

T distributions:

At low sample sizes, the t-distribution more accurately estimates the shape of the probability distribution. The t-distribution tends toward the normal distribution when the sample size is large.

As a rule of thumb, at less than 30 samples the sample mean and standard deviation does not accurately reflect the population mean.

```{r}
#Use dnorm() and dt() to plot the normal and t-distributions on the same graph

x <- seq(-4, 4, length=100)
hx <- dnorm(x)
degf <- c(1, 3, 8, 30)
colors <- c("red", "blue", "darkgreen", "gold", "black")
labels <- c("df=1", "df=3", "df=8", "df=30", "normal")
      
plot(x, hx, type="l", lty=2, xlab="x value",
        ylab="Density", main="Comparison of t Distributions")
for (i in 1:4){
lines(x, dt(x,degf[i]), lwd=2, col=colors[i])
}
      legend("topright", inset=.05, title="Distributions",
        labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)
```

p-value: The p-value is the probability that two or more sets of measurements are from the same phenomenon.

Z-Test:

models the population
```{r}
x1 = rnorm(31, mean = 4.5, sd = 0.2)

##Use the z.test() function to test whether x1 lies within a normally distributed
##population of mean 3 and standard deviation of 0.5

ztest <- z.test(x1, mu = 3, stdev = 0.5)

summary(ztest)
ztest
ztest$p.value
```

T-tests:

One-sample t-test: Used to compare a population mean to a t-distributed set of sample measurements
  
Two-sample t-test: The t-statistic and the t-distribution is used to compare two sets of sample measurements (n<30)

```{r}
x1 = c(2,3,2.4)

#Use t.test() to determine whether x1 is part of a population
#with a mean of 4. Find the t-statistic

t.test(x1, mu = 4)
```
T distribution models the sample so the  we use the sd from the sample to calculate it

Two-sample t-test:
 - most commonly used statistical test in biology
 - used to compare two t-distributed sets of measurements
    - two sets of measurements that have limited numbers of measurements so that they       can ce compared

```{r}
x1 = c(2,3,2.4)
x2 = c(4,5,8)

t.test(x1, x2)
```
The population means are not significantly different from 0

Two more examples:

```{r}
Test1 = t.test( c(8, 12, 9, 11), c(18, 19, 22, 21))
Test1
Test2 = t.test(c(8, 12, 9, 11), c(12, 12, 12))
Test2
```
test1 rejects the null, but test2 doesn't

```{r}
ctrlMoCApre <- c(24.75924, 26.98667, 24.94788, 27.73277, 26.32180, 24.33580, 23.93787)
ctrlMoCApost <- c(25.53444, 29.59317, 27.64380, 24.93487, 28.18750, 28.66826, 25.12095)

t.test(ctrlMoCApre, ctrlMoCApost)

vidMoCaPre <- c(27.04761, 27.04171, 28.94454, 26.86962, 25.55325, 27.40797, 26.10106)
vidMoCaPost <- c(30.00503, 29.29816, 27.00058, 28.77350, 29.11423, 30.94159, 28.30345)

t.test(vidMoCaPre, vidMoCaPost)
```

steps for two sample t-test:

• Consider two groups for which you obtain measurements.
• Make the null hypothesis (H0) that there is no difference in the means of these two groups.
• Make the alternate hypothesis (H1) that there is a difference.
• Calculate the t-statistic first: In the numerator take the absolute value of the difference of the two group means.
• In the denominator calculate the “noise”.
• From the t-statistic obtain a probability value.


Innacurate p-value
small sample size --> measurements not representative of the population

Type 1 error (alpha):
 - The probability that you reject the
null hypothesis when it is true

- In other words, you think there is a difference (low p-value), when there actually is not one

- The False Discovery Rate is the probability of committing Type 1 error, which is also the rate of false positives


Type 2 error (beta):
 - The probability that you will accept the
null hypothesis when it is false

 - In other words, you think there is no difference between data sets (p > 0.05), when there actually is a difference.

 - The Power is 1-beta, or the probability that you will reject a null hypothesis when it is false
 
False discovery rate:
The false discovery rate (FDR) is a popular multiple corrections correction. A false positive (also called a type I error) is sometimes called a false discovery.
FDR = number of false positives / number called significant

t-test: power calculation
- Power is the fraction of true positives that will be detected. It is a value between 0 and 1. The larger the sample size, the larger the power


```{r}
pwr.t.test(n = 22, sig.level = 0.05, d = 1, type = "two.sample")
```

