---
title: "R Programming Review"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Frequently missed questions
Using read.csv to find a particular file

~/ for finding a file only needed when you are looking in the home directory.
if you are in the working directory, then yo can just type the file name in quotes

Select the correct ways to  extract a vector from a row in the data frame

df[n, ]
df["rowname", ]

for columns:

df[, n]
df[, "colname"]
df$colname


What input data types can this function take?

jal <- function(x) {
 y = x^2
 return(y)
}

Can take either a numeric or logical value as input without returning error

Conversion between types of data tables in R
 as.dataframe()
 as.data.table()
 as.matrix()
 
 
minimum input for a single layer scatter plot (ie. plot())
  a single numeric vector
  
  this is also the minimum input for hist() and barplot()
  
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Review of statistics

Ho: the sea creature is the monster
Ha: sea creature is not

Null is that there is no difference between our hypothesis and the data

variance: (sigma)^2


Examples:

x = c(5, 6, 4, 6.5)
y = c(2, 10, 7, 3, 5)
x10 <- 10 *c(5, 6, 4, 6.5)

```{r}
x = c(5, 6, 4, 6.5)
y = c(2, 10, 7, 3, 5)
x10 <- 10 *c(5, 6, 4, 6.5)

all <- c(x, y, x10)

sd(x)
sd(y)
sd(x10)

sds
vars


```


What happens to the variance when I scale a measurement from 0→ 100 (mean = 50) to 0→ 1 (mean = 0.5)?

  variance decrases because the range of values is smaller.  Scaling down measurement causes variance to decrease.
  
How does scaling measurements allow for a better comparison of the variances between principal components?

  allows for easier comparison between PCs. let's you account for real variance in   the   data
  
Normal distributions
```{r}
vec <- seq(-1, 1, by = 0.01)
plot(dnorm(vec, mean = 5, sd = 2))

hist(rnorm(5000, mean = 5, sd = 2), breaks = 15)
hist(rnorm(10000, mean = 5, sd = 2), breaks = 30)
abline(hist(rnorm(10000, mean = 5, sd = 2), breaks = 30), v = c(3, 7), col = "red")
```


```{r}
abs(pnorm(2, 5, 2) - pnorm(6, 5, 2)) *100
```

BUild a function to plot the normal distribuition:
Sample size is a big obstacle in creating a normal dist. --> if you don't have enough data points then it is going to be very difficult to conclude that the normal distribution is a good representation of the population

```{r}
x = c(1:100)
mean = 50
stdev = 10
coolnorm <- function(x,mean,stdev){
      y = (1/stdev*sqrt(2*pi))*(exp(1))^(-((x-mean)^2)/(2*stdev)^2)
      return(y)
}
plot(coolnorm(x,mean,stdev))


```


COnfience interval
```{r}
norm <- hist(rnorm(10000, 5, 2))
abline(plot(norm), v = c(3, 7), col = "red")
```

T distributions:

At low sample sizes, the t-distribution more accurately estimates the shape of the probability distribution. The t-distribution tends toward the normal distribution when the sample size is large.

As a rule of thumb, at less than 30 samples the sample mean and standard deviation does not accurately reflect the population mean.

```{r}
#Use dnorm() and dt() to plot the normal and t-distributions on the same graph

x <- seq(-4, 4, length=100)
hx <- dnorm(x)
degf <- c(1, 3, 8, 30)
colors <- c("red", "blue", "darkgreen", "gold", "black")
labels <- c("df=1", "df=3", "df=8", "df=30", "normal")
      
plot(x, hx, type="l", lty=2, xlab="x value",
        ylab="Density", main="Comparison of t Distributions")
for (i in 1:4){
lines(x, dt(x,degf[i]), lwd=2, col=colors[i])
}
      legend("topright", inset=.05, title="Distributions",
        labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)
```

p-value: The p-value is the probability that two or more sets of measurements are from the same phenomenon.

Z-Test:
```{r}
x1 = rnorm(31, mean = 4.5, sd = 0.2)

##Use the z.test() function to test whether x1 lies within a normally distributed
##population of mean 3 and standard deviation of 0.5

ztest <- z.test(x1, mu = 3, stdev = 0.5)

summary(ztest)
ztest
ztest$p.value
```

